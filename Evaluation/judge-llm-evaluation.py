

openai_api_key = ""

from openai import OpenAI
import base64
from PIL import Image
import io
import os
from pathlib import Path
import json
import ast
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm



system_prompt = """ You are an expert evaluator trained in **linguistic and factual assessment**. Your task is to compare two textual descriptions of a **historical artifact**:  

- **Ground Truth:** The original, correct description.  
- **VLM Generated Description:** The description generated by a Vision-Language Model (VLM).  

Your goal is to assess **contextual, semantic, and factual similarity**, assigning a similarity score between **0 and 1** based on the following criteria:  

### **Evaluation Criteria:**  
1Ô∏è‚É£ **Semantic Meaning** ‚Äì Does the VLM response convey the same **core idea** as the Ground Truth?  
2Ô∏è‚É£ **Context Awareness** ‚Äì Does the response align with the **intended historical or cultural context**?  
3Ô∏è‚É£ **Factual Accuracy** ‚Äì Does the response contain **accurate historical details** without **hallucinated or misleading information**?  
4Ô∏è‚É£ **Handling Omissions & Additions** ‚Äì Is any **key information missing or incorrect**? If the VLM adds **extra details**, are they **factually correct**?  
5Ô∏è‚É£ **Grammar & Fluency (Minimal Impact)** ‚Äì Minor grammatical changes **should not** affect the score unless they change meaning.  

### **Scoring Guide (0 to 1):**  
- **0.0 ‚Üí No similarity** (Completely incorrect, factually unrelated).  
- **0.1 - 0.4 ‚Üí Weak similarity** (Contains major **factual errors**, missing key details, or **hallucinated content**).  
- **0.5 - 0.7 ‚Üí Moderate similarity** (Key elements are correct, but some **historical inaccuracies or missing context** exist).  
- **0.8 - 0.9 ‚Üí High similarity** (Mostly accurate, only **minor phrasing or non-critical omissions**).  
- **1.0 ‚Üí Perfect match** (Identical meaning, no **missing or misleading** details).  

### **Additional Rules:**  
- ‚úÖ **Minor rewordings are acceptable** if they **retain meaning**.  
- üö® **Hallucinated information** (incorrectly added details) **should be penalized**.  
- üîç If the artifact has **multiple valid interpretations**, do not penalize **historically accurate variations**.  

### **Input Format:**  
```
Ground Truth: <original description>  
VLM Generated Description: <description>
```

### **Output Format (Strictly JSON):**  
Your response must be a **valid JSON object**:  
```json
{"score": 0.85}
```
*(No additional text or explanation should be included in the output JSON.)*

**Ensure objectivity, factual accuracy, and consistency in scoring.**  

"""



from pydantic import BaseModel

class Similarity_Score(BaseModel):
    score: float



client = OpenAI(
    api_key=openai_api_key  
)

def compare_description(ground_truth_description,vlm_response):
    
    # img_data_url = pil_to_png(image_pil)

    messages = [
        {"role": "system", "content": system_prompt},
        {
            "role": "user",
            "content": [
                {"type": "text", "text": f"Ground Truth : {ground_truth_description}" + "\n\n\n" + f"VLM Response : {vlm_response}" },
                # {
                #     "type": "image_url",
                #     "image_url": {"url": img_data_url},
                # },
            ],
        }
    ]

    response = client.beta.chat.completions.parse(
        model="gpt-4o",  
        messages=messages,
        max_tokens=512,
        response_format=Similarity_Score,
    )

    return response.choices[0].message.content




ground_truth_file = "/data2/ketan/history_project_subset_v2_lite/datatset/ground_truth_description.json"
vlm_reponse_file = "/data2/ketan/history_project_subset_v2_lite/desciption_generation/response.json"
output_path = "/data2/ketan/history_project_subset_v2/judge_llm_evaluation/results/results.json"

with open(ground_truth_file) as flp :
    ground_truth_dataset = json.load(flp)


with open(vlm_reponse_file) as flp :
    vlm_reponse_dataset = json.load(flp)


ground_truth_dataset = sorted(ground_truth_dataset, key=lambda x: x["id"])
vlm_reponse_dataset = sorted(vlm_reponse_dataset, key=lambda x: x["id"])

# Check if both datasets have the same ids
gt_ids = {item["id"] for item in ground_truth_dataset}
vlm_ids = {item["id"] for item in vlm_reponse_dataset}

if gt_ids == vlm_ids:
    print("Both datasets have the exact same IDs.")
else:
    print("Mismatch in IDs between the datasets.")
    missing_in_gt = vlm_ids - gt_ids
    missing_in_vlm = gt_ids - vlm_ids
    print(f"IDs missing in ground truth dataset: {missing_in_gt}")
    print(f"IDs missing in VLM response dataset: {missing_in_vlm}")
    exit()






def process_pair(ground_truth, vlm_reponse):
    try:
        if ground_truth["id"] == vlm_reponse["id"]:
            score = ast.literal_eval(compare_description(ground_truth["description"], vlm_reponse["llava_next_description"]))["score"]
            return {"id": ground_truth["id"], "ground_truth": ground_truth["description"], "vlm_reponse": vlm_reponse["llava_next_description"], "Score": score}, score
        else:
            print("Fault:", ground_truth["id"])
            return None, None
    except Exception as e:
        print(f"Error processing ID {ground_truth['id']}: {e}")
        return None, None


def main():
    results = []
    scores = []
    
    with ThreadPoolExecutor() as executor:
        future_to_data = {executor.submit(process_pair, gt, vr): (gt, vr) for gt, vr in zip(ground_truth_dataset, vlm_reponse_dataset)}
        
        for future in tqdm(as_completed(future_to_data), total=len(future_to_data), desc="Processing Pairs"):
            result, score = future.result()
            if result:
                results.append(result)
                scores.append(score)

    # Save results
    with open(output_path, "w") as flp:
        json.dump(results, flp, indent=4)
    
    print(f"Results saved to {output_path}")

# if __name__ == "__main__":
main()


with open(output_path) as flp :
    results = json.load(flp)

scores = []
for item in results :
    scores.append(item["Score"])



numbers = scores
average = sum(numbers) / len(numbers)
print(round(average,4))